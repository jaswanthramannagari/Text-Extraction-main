{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddee502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ObjectDetection.SSD_utils import *\n",
    "import pandas as pd\n",
    "import fnmatch\n",
    "import string\n",
    "import joblib\n",
    "from cv2.ximgproc import guidedFilter\n",
    "from Word_separation import *;\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.activations import relu, sigmoid, softmax\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7106e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list = string.ascii_letters+string.digits\n",
    " \n",
    "def encode_to_labels(txt):\n",
    "    # encoding each output word into digits\n",
    "    dig_lst = []\n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            print(char)\n",
    "        \n",
    "    return dig_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e88a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input with shape of height=32 and width=128 \n",
    "inputs = Input(shape=(32,128,1))\n",
    "# convolution layer with kernel size (3,3)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "# poolig layer with kernel size (2,2)\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    " \n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    " \n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    " \n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "# poolig layer with kernel size (2,1)\n",
    "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    " \n",
    "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "# Batch normalization layer\n",
    "batch_norm_5 = BatchNormalization()(conv_5)\n",
    " \n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    " \n",
    "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    " \n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    " # bidirectional LSTM layers with units=128\n",
    "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    " \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "# model to be used at test time\n",
    "act_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d89aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text(img):\n",
    "    act_model.load_weights('best_model_v2.hdf5')\n",
    "\n",
    "    # predict outputs on validation images\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    img = cv2.resize(img, (128,32))\n",
    "    img = np.expand_dims(img , axis = 2)\n",
    "    img = img.reshape(1,32,128,1)\n",
    "    prediction = act_model.predict(img)\n",
    "\n",
    "    # use CTC decoder\n",
    "    out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
    "                             greedy=True)[0][0])\n",
    "    res = \"\"\n",
    "    for p in out[0]:  \n",
    "            if int(p) != -1:\n",
    "                res+=(char_list[int(p)])\n",
    "    return res\n",
    "\n",
    "def text_extraction(image):\n",
    "    out = []\n",
    "    orig = image.copy()\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 18)\n",
    "    edges = edge_detect(blurred)\n",
    "    ret, edges = cv2.threshold(edges, 50, 255, cv2.THRESH_BINARY)\n",
    "    bw_image = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, np.ones((20,20), np.uint8))\n",
    "    \n",
    "    boxes = text_detect(bw_image, image)\n",
    "    lines = sort_words(boxes)\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            guided = guidedFilter(orig,orig,8,0.05)    \n",
    "            segmentation(orig)\n",
    "            img = cv2.imread('seg.jpg')\n",
    "            res = test_text(img[y1:y2,x1:x2])\n",
    "            out.append(res)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c83911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_detection(image, interpreter):\n",
    "    # Run model: start to detect\n",
    "    # Sets the value of the input tensor.\n",
    "    interpreter.set_tensor(input_details[0]['index'], image)\n",
    "    # Invoke the interpreter.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # get results\n",
    "    boxes = interpreter.get_tensor(output_details[0]['index'])\n",
    "    classes = interpreter.get_tensor(output_details[1]['index'])\n",
    "    scores = interpreter.get_tensor(output_details[2]['index'])\n",
    "    num = interpreter.get_tensor(output_details[3]['index'])\n",
    "\n",
    "    boxes, scores, classes = np.squeeze(boxes), np.squeeze(scores), np.squeeze(classes + 1).astype(np.int32)\n",
    "    out_scores, out_boxes, out_classes = non_max_suppression(scores, boxes, classes)\n",
    "\n",
    "    # Print predictions info\n",
    "    #print('Found {} boxes for {}'.format(len(out_boxes), 'images/dog.jpg'))\n",
    "            \n",
    "    return out_scores, out_boxes, out_classes\n",
    "\n",
    "def image_object_detection(interpreter, colors,img):\n",
    "    #image = cv2.imread('dog.jpg')\n",
    "    image_data = preprocess_image_for_tflite(img, model_image_size=300)\n",
    "    out_scores, out_boxes, out_classes = run_detection(image_data, interpreter)\n",
    "\n",
    "    # Draw bounding boxes on the image file\n",
    "    #result = draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n",
    "    # Save the predicted bounding box on the image\n",
    "    #cv2.imwrite(os.path.join(\"out\", \"ssdlite_mobilenet_v2_dog.jpg\"), result, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "    res = []\n",
    "    for i, c in reversed(list(enumerate(out_classes))):\n",
    "        res.append(class_names[c])\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4d4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(img):\n",
    "    b,g,r = cv2.split(img)\n",
    "    rgb_img = cv2.merge([r,g,b])\n",
    "\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    \n",
    "    cv2.imwrite('seg.jpg',thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b481477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person']\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('image13.jpg')\n",
    "orig = img.copy()\n",
    "img = cv2.resize(img, (220, 220))\n",
    "img = np.reshape(img, (145200))\n",
    "img = img.reshape(1,-1)\n",
    "\n",
    "gnb = joblib.load('tnt_class.pkl')\n",
    "out = gnb.predict(img)\n",
    "\n",
    "img = orig.copy()\n",
    "\n",
    "if out == 0:\n",
    "    res = text_extraction(orig)\n",
    "else:\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"ObjectDetection/model_data/ssdlite_mobilenet_v2.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # label\n",
    "    class_names = read_classes('ObjectDetection/model_data/coco_classes.txt')\n",
    "    # Generate colors for drawing bounding boxes.\n",
    "    colors = generate_colors(class_names)\n",
    "            \n",
    "    #image_object_detection(interpreter, colors)\n",
    "    res = image_object_detection(interpreter, colors,orig)\n",
    "    \n",
    "    if res == []:\n",
    "        res = text_extraction(orig)\n",
    "    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c73ab64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "for i in range(len(res)):\n",
    "    engine.say(res[i])\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50f101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
